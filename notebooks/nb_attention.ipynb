{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Attention without Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([1, 1, 4, 64])\n",
      "Attention Scores Shape: torch.Size([1, 1, 4, 4])\n",
      "Attention Scores:\n",
      "tensor([[[[0.3822, 0.2345, 0.2504, 0.2440],\n",
      "          [0.1791, 0.4443, 0.2518, 0.2360],\n",
      "          [0.1597, 0.0000, 0.4912, 0.2500],\n",
      "          [0.1708, 0.2163, 0.2745, 0.4495]]]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer.layer import MultiHeadAttention\n",
    "# Suppose we have a batch of 1 sequences (mini-batch size of 1)\n",
    "# Each sequence has 4 words (sequence length of 4)\n",
    "# We use 1 attention heads (h = 1) and the dimension of key/query (d_k) is 64\n",
    "batch_size = 1\n",
    "h = 1\n",
    "seq_len = 4\n",
    "d_k = 64\n",
    "\n",
    "torch.manual_seed(68) # for reproducible result of random process\n",
    "input_tensor = torch.rand(batch_size, h, seq_len, d_k)\n",
    "query_k = input_tensor.clone()\n",
    "key_k = input_tensor.clone()\n",
    "value_k = input_tensor.clone()\n",
    "\n",
    "# Call the attention function\n",
    "output, attention_scores = MultiHeadAttention.attention(query_k, key_k, value_k, d_k, mask=None, dropout=nn.Dropout(0.1))\n",
    "\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Attention Scores Shape: {attention_scores.shape}\")\n",
    "print(f\"Attention Scores:\\n{attention_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Attention with Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Mask:\n",
      "tensor([[[1, 0, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0]]], dtype=torch.int32)\n",
      "\n",
      "Output Shape: torch.Size([1, 1, 4, 64])\n",
      "Attention Scores Shape: torch.Size([1, 1, 4, 4])\n",
      "Attention Scores:\n",
      "tensor([[[[1.1111, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3191, 0.7920, 0.0000, 0.0000],\n",
      "          [0.4796, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4902, 0.6209, 0.0000, 0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer.layer import MultiHeadAttention\n",
    "from transformer.mask import create_decoder_mask\n",
    "# Suppose we have a batch of 1 sequences (mini-batch size of 1)\n",
    "# Each sequence has 4 words (sequence length of 4)\n",
    "# We use 1 attention heads (h = 1) and the dimension of key/query (d_k) is 64\n",
    "batch_size = 1\n",
    "h = 1\n",
    "seq_len = 4\n",
    "d_k = 64\n",
    "pad_token_id = torch.tensor([0]) # padding token ID\n",
    "\n",
    "torch.manual_seed(68) # for reproducible result of random process\n",
    "input_tensor = torch.rand(batch_size, h, seq_len, d_k)\n",
    "query_k = input_tensor.clone()\n",
    "key_k = input_tensor.clone()\n",
    "value_k = input_tensor.clone()\n",
    "\n",
    "# Create decoder mask\n",
    "decoder_input_ids = torch.tensor([ 2, 68, 0, 0])\n",
    "pad_token_id = torch.tensor([0]) # padding token ID\n",
    "decoder_mask = create_decoder_mask(decoder_input_ids, pad_token_id, seq_len)\n",
    "\n",
    "# Call the attention function\n",
    "output, attention_scores = MultiHeadAttention.attention(query_k, key_k, value_k, d_k, mask=decoder_mask, dropout=nn.Dropout(0.1))\n",
    "\n",
    "print(f\"Decoder Mask:\\n{decoder_mask}\\n\")\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(f\"Attention Scores Shape: {attention_scores.shape}\")\n",
    "print(f\"Attention Scores:\\n{attention_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
