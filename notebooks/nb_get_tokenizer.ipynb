{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'en': '$10,000 Gold?', 'fr': 'L’or à 10.000 dollars l’once\\xa0?'}, {'en': 'SAN FRANCISCO – It has never been easy to have a rational conversation about the value of gold.', 'fr': 'SAN FRANCISCO – Il n’a jamais été facile d’avoir une discussion rationnelle sur la valeur du métal jaune.'}]\n",
      "Source vocabulary size:  30000\n",
      "Target vocabulary size:  30000\n",
      "Source tokenizer encodes 'I love you':  [131, 3181, 345]\n",
      "Source tokenizer decodes [131, 3181, 345]:  I love you\n",
      "Target tokenizer encodes 'Je vais bien':  [783, 11957, 70]\n",
      "Target tokenizer decodes [783, 11957, 70]:  Je vais bien\n",
      "Source tokenizer encodes'Love': [17854]\n",
      "Source tokenizer encodes 'love':  [3181]\n",
      "Source tokenizer encodes'Love':  17854\n",
      "Source tokenizer encodes'love':  3181\n",
      "Source tokenizer decodes 17854: Love\n",
      "Source tokenizer decodes 3181: love\n",
      "Execution time: 41.480934619903564 seconds\n",
      "Maximum length of source sentences:  222\n",
      "Maximum length of target sentences:  348\n",
      "SOS token id:  [2]\n",
      "EOS token id:  [3]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from transformer.utils import load_config, get_dataset, get_tokenizer, calculate_max_lengths\n",
    "\n",
    "## Load config\n",
    "config_file_path = \"../config.json\"\n",
    "config = load_config(config_file_path)\n",
    "\n",
    "# Load dataset\n",
    "dataset = get_dataset(config)\n",
    "print(dataset['translation'][:2])\n",
    "\n",
    "## Load tokenizer\n",
    "tokenizer_src = get_tokenizer(config, dataset, config['language_source'])\n",
    "tokenizer_tgt = get_tokenizer(config, dataset, config['language_target'])\n",
    "\n",
    "## Explore tokenizer\n",
    "# check vocabulary size\n",
    "print(\"Source vocabulary size: \", tokenizer_src.get_vocab_size())\n",
    "print(\"Target vocabulary size: \", tokenizer_tgt.get_vocab_size()) \n",
    "\n",
    "# Check source tokenizer encodes/decodes\n",
    "print(\"Source tokenizer encodes 'I love you': \", tokenizer_src.encode('I love you').ids)\n",
    "print(\"Source tokenizer decodes [131, 3181, 345]: \", tokenizer_src.decode([131, 3181, 345])) \n",
    "\n",
    "# Check source tokenizer encodes/decodes\n",
    "print(\"Target tokenizer encodes 'Je vais bien': \", tokenizer_tgt.encode('Je vais bien' ).ids)\n",
    "print(\"Target tokenizer decodes [783, 11957, 70]: \", tokenizer_tgt.decode([783, 11957, 70])) \n",
    "\n",
    "# Comparing Uppercase anc Lowercase words\n",
    "print(\"Source tokenizer encodes'Love':\", tokenizer_src.encode('Love').ids)\n",
    "print(\"Source tokenizer encodes 'love': \", tokenizer_src.encode('love').ids)\n",
    "\n",
    "# check token_to_id method\n",
    "print(\"Source tokenizer encodes'Love': \", tokenizer_src.token_to_id('Love'))\n",
    "print(\"Source tokenizer encodes'love': \", tokenizer_src.token_to_id('love'))\n",
    "\n",
    "# check id_to_token method\n",
    "print(\"Source tokenizer decodes 17854:\", tokenizer_src.id_to_token(17854))\n",
    "print(\"Source tokenizer decodes 3181:\", tokenizer_src.id_to_token(3181))\n",
    "\n",
    "## Check maximum length of source and target sequences\n",
    "# Use the function\n",
    "max_src_len, max_tgt_len = calculate_max_lengths(dataset, tokenizer_src, tokenizer_tgt, config)\n",
    "print(\"Maximum length of source sentences: \", max_src_len)\n",
    "print(\"Maximum length of target sentences: \", max_tgt_len)\n",
    "print(\"SOS token id: \",tokenizer_src.encode('[SOS]').ids)\n",
    "print(\"EOS token id: \",tokenizer_src.encode('[EOS]').ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part3-trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
